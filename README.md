# Interpretable ML

Code for trying out several techniques for the explainability of models. 
I explored Partial Dependency Plots, Local Interpretable Model-agnostic Explanations (LIME), and 
SHapley Additive exPlanations (SHAP). 

Due to the nature of the DPD, LIME and SHAP packages that I used the use of jupyter notebooks was chosen as a nice way to explain the models. 

**To do:**
* Show how you can use SHAP values for deploying models
