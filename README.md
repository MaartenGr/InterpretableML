# Interpretable ML

Code for trying out several techniques for the explainability of models. 
I explored Partial Dependency Plots, Local Interpretable Model-agnostic Explanations (LIME), and 
SHapley Additive exPlanations (SHAP). 

Due to the nature of the DPD, LIME and SHAP packages that I used the use of jupyter notebooks was chosen as a 
nice way to explain the models. 

<img src="https://github.com/MaartenGr/InterpretableML/blob/master/Images/lime.PNG" width="70%"/>
<img src="https://github.com/MaartenGr/InterpretableML/blob/master/Images/occupation.png" width="70%"/>
<img src="https://github.com/MaartenGr/InterpretableML/blob/master/Images/shap.PNG" width="70%"/>
<img src="https://github.com/MaartenGr/InterpretableML/blob/master/Images/summary_shap.png" width="70%"/>

**To do:**
* Show how you can use SHAP values for deploying models
